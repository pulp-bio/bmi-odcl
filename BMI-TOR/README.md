# BMI-TOR

This repository contains implementation of the Train-On-Request (TOR) workflow based on **[QuantLab](https://github.com/pulp-platform/quantlab/tree/main)**. 

Please cite the following publication if you use our TOR implementation:

```
@misc{mei2024trainonrequestondevicecontinuallearning,
      title={Train-On-Request: An On-Device Continual Learning Workflow for Adaptive Real-World Brain Machine Interfaces}, 
      author={Lan Mei, Cristian Cioflan, Thorir Mar Ingolfsson, Victor Kartsch, Andrea Cossettini, Xiaying Wang, Luca Benini},
      year={2024},
      eprint={2409.09161},
      archivePrefix={arXiv},
      primaryClass={eess.SP},
      url={https://arxiv.org/abs/2409.09161}, 
}
```

## Datasets and Usage
### Datasets
The TOR implementation is based on an in-house EEG dataset, [Dataset A](https://iis-people.ee.ethz.ch/~datasets/Datasets-ODCL/DatasetA/). 

To run the TOR experiments, after downloading the dataset, put all csv files (from S1 to S7) into the `systems/BCI_Drone_4classes/data/DatasetA_full_data` folder, and put all csv files from S2 to S7 (excluding csv files in `DatasetA_S1_1127`) into the `systems/BCI_Drone_4classes/data/DatasetA_full_data_exc_first` folder.

### Usage
#### Configuration and Setup

To configure QuantLab, first configure where the data for and logs of your experiments should be physically stored by editing `storage_cfg.json`:
```
(pytorch-1.13) $ vim BMI-TOR/configure/storage_cfg.json
$ 
{
    'data': '/scratchb',
    'logs': '/scratcha'
}
:wq
(pytorch-1.13) $ 
```
In this case, the tool will fetch your data from the SSD drive and write logs to the HDD drive. Note that you must specify absolute (not relative) paths.
Running the `storage.sh` script creates *mock-up QuantLab homes* under both folders:
```
(pytorch-1.13) $ bash configure/storage.sh
```

QuantLab is shipped with example *problem packages* (in our case, `BCI_Drone_4classes`). Our problem package `BCI_Drone_4classes` contains a *topology sub-package*, i.e., `MIBMINet`. For this implementation, we use MI-BMINet for classifying BCI Dataset A. 
When running an experiment in QuantLab, its abstractions will look for data in a `data` sub-folder under the chosen problem package, independently of the chosen network topology.
Instead, they will log results in a `logs` sub-folder under the chosen topology sub-package.
The `problem.sh` and `topology.sh` scripts actually create such folders on the devices specified at configuration time, and then create links to these folders under the problem and topology sub-package:
```
(pytorch-1.13) $ bash configure/problem.sh BCI_Drone_4classes       
(pytorch-1.13) $ bash configure/topology.sh BCI_Drone_4classes MIBMINet  
```

#### Running Experiments

For TOR-TL, train the network with:
```
(pytorch-1.13) $ python main.py --problem=BCI_Drone_4classes --topology=MIBMINet train_TL_on_request_withPretrained_prev_sessions_long_chain_test_acc --exp_id=989
```

For TOR-LwF, train the network with:
```
(pytorch-1.13) $ python main.py --problem=BCI_Drone_4classes --topology=MIBMINet train_LwF_on_request_withPretrained_prev_sessions_long_chain_test_acc --exp_id=990
```

For TOR-ER, train the network with:
```
(pytorch-1.13) $ python main.py --problem=BCI_Drone_4classes --topology=MIBMINet train_ER_on_request_withPretrained_prev_sessions_long_chain_test_acc --exp_id=991
```

There is a log folder `exp0001` in `systems/BCI_Drone_4classes/MIBMINet/logs` which is generated by obtaining the pretrained model through training on only the first session. It serves as a pretrained model for the TOR experiments. After putting the first session data folder `DatasetA_S1_1127` inside `systems/BCI_Drone_4classes/data`, the pretrained model can be reproduced by: 
```
(pytorch-1.13) $ python main.py --problem=BCI_Drone_4classes --topology=MIBMINet configure    # Obtained exp_id=XX
(pytorch-1.13) $ python main.py --problem=BCI_Drone_4classes --topology=MIBMINet train --exp_id=XX    # Train on newly-configured exp_id=XX
```

Hyperparameters, e.g., learning rate and number of epochs, can be adjusted by modifying the corresponding `config.json` of each TOR experiment. 

To configure and run a new TOR experiment with possibly different hyperparameters, create a new log folder, paste the corresponding `config.json` of the type of experiment you wish to run (TOR-TL, TOR-LwF or TOR-ER), modify the parameters in `config.json`, and train the network as introduced before with the new `exp_id`.

QuantLab depends on [TensorBoard](https://www.tensorflow.org/tensorboard) for enabling useful analysis and data visualisations.
After a training run has reached completion, you can inspect the logged statistics by issuing the following command:
```
(pytorch-1.13) $ tensorboard --log_dir=~/BMI-TOR/systems/BCI_Drone_4classes/MIBMINet/logs/exp0992 --port=6006
```

In our case, printed results obtained for the paper are stored manually in `results_note.txt`.

## Contributors
* Lan Mei, ETH Zurich lanmei@student.ethz.ch
* Thorir Mar Ingolfsson, ETH Zurich thoriri@iis.ee.ethz.ch
* Cristian Cioflan, ETH Zurich cioflanc@iis.ee.ethz.ch
* Victor Kartsch, ETH Zurich victor.kartsch@iis.ee.ethz.ch
* Andrea Cossettini, ETH Zurich cossettini.andrea@iis.ee.ethz.ch
* Xiaying Wang, ETH Zurich xiaywang@iis.ee.ethz.ch

## Acknowledgement
The early-stopping implementation is based on: [early-stopping-pytorch](https://github.com/Bjarten/early-stopping-pytorch/tree/master?tab=MIT-1-ov-file) with the [MIT License](https://github.com/Bjarten/early-stopping-pytorch/blob/master/LICENSE).

## License
Unless explicitly stated otherwise, the code is released under Apache 2.0. Please see the LICENSE file in the root of this repository for details. Note that the license under which the current repository is released might differ from the license of each individual package:

* Avalanche - [MIT License](https://github.com/ContinualAI/avalanche/blob/master/LICENSE);
* PyTorch - a [mix of licenses](https://github.com/pytorch/pytorch/blob/master/NOTICE), including the Apache 2.0 License and the 3-Clause BSD License;
* TensorBoard - [Apache 2.0 License](https://github.com/tensorflow/tensorboard/blob/master/LICENSE);
* NetworkX - [3-Clause BSD License](https://github.com/networkx/networkx/blob/main/LICENSE.txt);
* GraphViz - [MIT License](https://github.com/graphp/graphviz/blob/master/LICENSE);
* matplotlib - a [custom license](https://github.com/matplotlib/matplotlib/blob/master/LICENSE/LICENSE);
* NumPy - [3-Clause BSD License](https://github.com/numpy/numpy/blob/main/LICENSE.txt);
* SciPy - [3-Clause BSD License](https://github.com/scipy/scipy/blob/master/LICENSE.txt);
* Mako - [MIT License](https://github.com/sqlalchemy/mako/blob/master/LICENSE);
* Jupyter - [3-Clause BSD License](https://github.com/jupyter/notebook/blob/master/LICENSE);
* Pandas - [3-Clause BSD License](https://github.com/pandas-dev/pandas/blob/main/LICENSE);
* early-stopping-pytorch - [MIT License](https://github.com/Bjarten/early-stopping-pytorch/blob/master/LICENSE).