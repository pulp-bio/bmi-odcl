# On-Device-Implementation

This repository contains on-device implementation of transfer learning & continual learning on the ultra-low power GAP9 microcontroller on the basis of [DORY](https://github.com/pulp-platform/dory) and [PULP-TrainLib](https://github.com/pulp-platform/pulp-trainlib/tree/main).

Please cite the following publication if you use our implementation(s):

```
@misc{mei2024BMIODCL,
      title={An Ultra-Low Power Wearable BMI System with Continual Learning Capabilities}, 
      author={Lan Mei, Thorir Mar Ingolfsson, Cristian Cioflan, Victor Kartsch, Andrea Cossettini, Xiaying Wang, Luca Benini},
      journal={IEEE Transactions on Biomedical Circuits and Systems}
      year={2024},
      doi={10.1109/TBCAS.2024.3457522}
}
```


## Structure
This implementation mainly contains:
* `Backbone-Example/`: An example of an a pretrained MI-BMINet backbone model (i.e., `model.onnx`), exported from [QuantLab](https://github.com/pulp-platform/quantlab) and the code generated by [DORY](https://github.com/pulp-platform/dory). The directory contains the integerized, frozen layers of the network except the fully-connected layer, as well as 100 samples from Dataset B. To generate such a backbone, please follow the "Exporting and Converting Quantized Network" section of [Offline-Training's README](../Offline-Training/README.md).
* `Classifier-Example/`: An example of the pretrained fully-connected layer of the MI-BMINet in FP32 with cross entropy loss, along with functionalities of forward and backward propagation based on [PULP-TrainLib](https://github.com/pulp-platform/pulp-trainlib/tree/main). The example also contains the intermediate activations generated by the backbone using 100 samples from Dataset B. To generate yourself `Classifier-Example/linear-data.h`, run `Classifier-Example/utils/GM_multi_inputs.py` with the input `*.npy` files, which creates a golden model for verification. 
* `Classifier-Example-LwF/`: An example of the pretrained fully-connected layer of the MI-BMINet in FP32 with LwF capabilities, along with functionalities of forward and backward propagation based on [PULP-TrainLib](https://github.com/pulp-platform/pulp-trainlib/tree/main). The example also contains the intermediate activations generated by the backbone using 100 samples from Dataset B. To generate yourself `Classifier-Example-LwF/linear-data.h`, run `Classifier-Example-LwF/utils/GM_multi_inputs.py` with the input `*.npy` files, which creates a golden model for verification. 
* `pulp-trainlib/`: Submodule [PULP-TrainLib](https://github.com/pulp-platform/pulp-trainlib/tree/main) based on `commit: 6615e084738958890ea9dd10195f8bbfe089ceb7` that is used for our implementation.
* `main.c`: Implementation of TL/ER with cross entropy loss, main function that connects frozen INT8 layers and the trainable fully-connected layer. A basic training and validation workflow.
* `main_LwF.c`: Implementation of LwF, main function that connects frozen INT8 layers and the trainable fully-connected layer. A basic training and validation workflow.
* The rest of the files include configuration files for running GAP9-SDK and files for building and compilation.

## Usage
To run the on-device learning workflow with cross entropy loss, run
```
cmake_script_run_net.sh
```

To run the on-device learning workflow with LwF, replace `main.c` with `main_LwF.c`, and replace `Classifier-Example` with `Classifier-Example-LwF` in `CMakeLists.txt`.

## Contributors
* Lan Mei, ETH Zurich lanmei@student.ethz.ch
* Thorir Mar Ingolfsson, ETH Zurich thoriri@iis.ee.ethz.ch
* Cristian Cioflan, ETH Zurich cioflanc@iis.ee.ethz.ch
* Victor Kartsch, ETH Zurich victor.kartsch@iis.ee.ethz.ch
* Andrea Cossettini, ETH Zurich cossettini.andrea@iis.ee.ethz.ch
* Xiaying Wang, ETH Zurich xiaywang@iis.ee.ethz.ch

## License
Unless explicitly stated otherwise, the code is released under Apache 2.0. Please see the LICENSE file in the root of this repository for details. 

As an exception, the weights:
* `./Backbone-Example/model.onnx`
* `./Backbone-Example/hex/*weights.hex`
* `./Classifier-Example/linear-data.h`
* `./Classifier-Example/weights_fc.npy` and `./Classifier-Example/bias_fc.npy`
* `./Classifier-Example-LwF/linear-data.h` 
* `./Classifier-Example-LwF/weights_fc.npy` and `./Classifier-Example-LwF/bias_fc.npy`

and the inputs:
* `./Backbone-Example/hex/*inputs.hex`
* `./Classifier-Example/inputs/`
* `./Classifier-Example-LwF/inputs/`

are released under Creative Commons Attribution-NoDerivatives 4.0 International. Please see the LICENSE file in their respective directories. 

Note that the license under which the current repository is released might differ from the license of each individual package:

* Avalanche - [MIT License](https://github.com/ContinualAI/avalanche/blob/master/LICENSE);
* PyTorch - a [mix of licenses](https://github.com/pytorch/pytorch/blob/master/NOTICE), including the Apache 2.0 License and the 3-Clause BSD License;
* TensorBoard - [Apache 2.0 License](https://github.com/tensorflow/tensorboard/blob/master/LICENSE);
* NetworkX - [3-Clause BSD License](https://github.com/networkx/networkx/blob/main/LICENSE.txt);
* GraphViz - [MIT License](https://github.com/graphp/graphviz/blob/master/LICENSE);
* matplotlib - a [custom license](https://github.com/matplotlib/matplotlib/blob/master/LICENSE/LICENSE);
* NumPy - [3-Clause BSD License](https://github.com/numpy/numpy/blob/main/LICENSE.txt);
* SciPy - [3-Clause BSD License](https://github.com/scipy/scipy/blob/master/LICENSE.txt);
* Mako - [MIT License](https://github.com/sqlalchemy/mako/blob/master/LICENSE);
* Jupyter - [3-Clause BSD License](https://github.com/jupyter/notebook/blob/master/LICENSE);
* Pandas - [3-Clause BSD License](https://github.com/pandas-dev/pandas/blob/main/LICENSE);
* early-stopping-pytorch - [MIT License](https://github.com/Bjarten/early-stopping-pytorch/blob/master/LICENSE).